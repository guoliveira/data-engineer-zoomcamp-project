{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02f2db32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "431e8382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-02-28 09:16:46--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/f6895842-79e6-4a43-9458-e5b0b454a340?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230228%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230228T091647Z&X-Amz-Expires=300&X-Amz-Signature=913d022ae0e2f628dfdbd6d4c2d7b936d51b859f4e5194d9ce1ff765bb07ced9&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dyellow_tripdata_2021-01.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2023-02-28 09:16:47--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/f6895842-79e6-4a43-9458-e5b0b454a340?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230228%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230228T091647Z&X-Amz-Expires=300&X-Amz-Signature=913d022ae0e2f628dfdbd6d4c2d7b936d51b859f4e5194d9ce1ff765bb07ced9&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dyellow_tripdata_2021-01.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 25031880 (24M) [application/octet-stream]\n",
      "Saving to: ‘yellow_tripdata_2021-01.csv.gz’\n",
      "\n",
      "yellow_tripdata_202 100%[===================>]  23.87M  42.0MB/s    in 0.6s    \n",
      "\n",
      "2023-02-28 09:16:48 (42.0 MB/s) - ‘yellow_tripdata_2021-01.csv.gz’ saved [25031880/25031880]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dde2525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbschema='dbt_dev,public'\n",
    "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi'\n",
    "                       , connect_args={'options': '-csearch_path={}'.format(dbschema)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55e61f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iter = pd.read_csv('yellow_tripdata_2021-01.csv.gz', iterator=True, chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fa21d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = next(df_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0727fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63177d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=0).to_sql(name='yellow_taxi_data', con=engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "befc6c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caa4d86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted another chunk, took 1.375 second\n",
      "inserted another chunk, took 1.227 second\n",
      "inserted another chunk, took 1.151 second\n",
      "inserted another chunk, took 2.483 second\n",
      "inserted another chunk, took 1.861 second\n",
      "inserted another chunk, took 1.743 second\n",
      "inserted another chunk, took 1.626 second\n",
      "inserted another chunk, took 1.265 second\n",
      "inserted another chunk, took 1.003 second\n",
      "inserted another chunk, took 1.039 second\n",
      "inserted another chunk, took 1.048 second\n",
      "inserted another chunk, took 1.293 second\n",
      "inserted another chunk, took 1.193 second\n",
      "inserted another chunk, took 1.231 second\n",
      "inserted another chunk, took 1.018 second\n",
      "inserted another chunk, took 1.611 second\n",
      "inserted another chunk, took 1.888 second\n",
      "inserted another chunk, took 1.044 second\n",
      "inserted another chunk, took 1.100 second\n",
      "inserted another chunk, took 1.444 second\n",
      "inserted another chunk, took 1.109 second\n",
      "inserted another chunk, took 1.075 second\n",
      "inserted another chunk, took 1.010 second\n",
      "inserted another chunk, took 1.033 second\n",
      "inserted another chunk, took 1.232 second\n",
      "inserted another chunk, took 1.178 second\n",
      "inserted another chunk, took 2.176 second\n",
      "inserted another chunk, took 2.145 second\n",
      "inserted another chunk, took 1.644 second\n",
      "inserted another chunk, took 1.164 second\n",
      "inserted another chunk, took 1.124 second\n",
      "inserted another chunk, took 1.526 second\n",
      "inserted another chunk, took 1.030 second\n",
      "inserted another chunk, took 1.093 second\n",
      "inserted another chunk, took 1.100 second\n",
      "inserted another chunk, took 1.244 second\n",
      "inserted another chunk, took 1.832 second\n",
      "inserted another chunk, took 1.917 second\n",
      "inserted another chunk, took 1.209 second\n",
      "inserted another chunk, took 1.211 second\n",
      "inserted another chunk, took 1.073 second\n",
      "inserted another chunk, took 1.110 second\n",
      "inserted another chunk, took 1.186 second\n",
      "inserted another chunk, took 1.329 second\n",
      "inserted another chunk, took 1.247 second\n",
      "inserted another chunk, took 1.026 second\n",
      "inserted another chunk, took 1.472 second\n",
      "inserted another chunk, took 2.087 second\n",
      "inserted another chunk, took 1.291 second\n",
      "inserted another chunk, took 1.118 second\n",
      "inserted another chunk, took 1.399 second\n",
      "inserted another chunk, took 1.251 second\n",
      "inserted another chunk, took 1.885 second\n",
      "inserted another chunk, took 1.475 second\n",
      "inserted another chunk, took 1.140 second\n",
      "inserted another chunk, took 1.752 second\n",
      "inserted another chunk, took 1.461 second\n",
      "inserted another chunk, took 1.632 second\n",
      "inserted another chunk, took 1.772 second\n",
      "inserted another chunk, took 1.548 second\n",
      "inserted another chunk, took 1.198 second\n",
      "inserted another chunk, took 1.098 second\n",
      "inserted another chunk, took 1.291 second\n",
      "inserted another chunk, took 1.042 second\n",
      "inserted another chunk, took 1.162 second\n",
      "inserted another chunk, took 1.257 second\n",
      "inserted another chunk, took 1.150 second\n",
      "inserted another chunk, took 1.318 second\n",
      "inserted another chunk, took 1.664 second\n",
      "inserted another chunk, took 1.920 second\n",
      "inserted another chunk, took 1.302 second\n",
      "inserted another chunk, took 1.019 second\n",
      "inserted another chunk, took 1.383 second\n",
      "inserted another chunk, took 1.055 second\n",
      "inserted another chunk, took 1.449 second\n",
      "inserted another chunk, took 1.394 second\n",
      "inserted another chunk, took 1.234 second\n",
      "inserted another chunk, took 1.321 second\n",
      "inserted another chunk, took 1.976 second\n",
      "inserted another chunk, took 2.062 second\n",
      "inserted another chunk, took 1.517 second\n",
      "inserted another chunk, took 1.325 second\n",
      "inserted another chunk, took 1.602 second\n",
      "inserted another chunk, took 1.484 second\n",
      "inserted another chunk, took 1.021 second\n",
      "inserted another chunk, took 1.105 second\n",
      "inserted another chunk, took 1.384 second\n",
      "inserted another chunk, took 1.218 second\n",
      "inserted another chunk, took 1.098 second\n",
      "inserted another chunk, took 1.096 second\n",
      "inserted another chunk, took 1.370 second\n",
      "inserted another chunk, took 1.076 second\n",
      "inserted another chunk, took 1.473 second\n",
      "inserted another chunk, took 2.037 second\n",
      "inserted another chunk, took 1.893 second\n",
      "inserted another chunk, took 1.238 second\n",
      "inserted another chunk, took 1.313 second\n",
      "inserted another chunk, took 1.285 second\n",
      "inserted another chunk, took 1.266 second\n",
      "inserted another chunk, took 1.184 second\n",
      "inserted another chunk, took 1.053 second\n",
      "inserted another chunk, took 1.128 second\n",
      "inserted another chunk, took 1.492 second\n",
      "inserted another chunk, took 2.207 second\n",
      "inserted another chunk, took 2.493 second\n",
      "inserted another chunk, took 1.387 second\n",
      "inserted another chunk, took 1.384 second\n",
      "inserted another chunk, took 1.735 second\n",
      "inserted another chunk, took 1.093 second\n",
      "inserted another chunk, took 1.362 second\n",
      "inserted another chunk, took 1.284 second\n",
      "inserted another chunk, took 1.129 second\n",
      "inserted another chunk, took 1.256 second\n",
      "inserted another chunk, took 1.195 second\n",
      "inserted another chunk, took 1.424 second\n",
      "inserted another chunk, took 1.732 second\n",
      "inserted another chunk, took 2.137 second\n",
      "inserted another chunk, took 1.117 second\n",
      "inserted another chunk, took 1.163 second\n",
      "inserted another chunk, took 1.314 second\n",
      "inserted another chunk, took 1.022 second\n",
      "inserted another chunk, took 1.186 second\n",
      "inserted another chunk, took 1.172 second\n",
      "inserted another chunk, took 1.095 second\n",
      "inserted another chunk, took 1.156 second\n",
      "inserted another chunk, took 1.714 second\n",
      "inserted another chunk, took 2.199 second\n",
      "inserted another chunk, took 1.240 second\n",
      "inserted another chunk, took 0.919 second\n",
      "inserted another chunk, took 1.099 second\n",
      "inserted another chunk, took 1.334 second\n",
      "inserted another chunk, took 1.329 second\n",
      "inserted another chunk, took 1.161 second\n",
      "inserted another chunk, took 1.833 second\n",
      "inserted another chunk, took 1.331 second\n",
      "inserted another chunk, took 1.389 second\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m: \n\u001b[1;32m      2\u001b[0m     t_start \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m----> 4\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     df\u001b[38;5;241m.\u001b[39mtpep_pickup_datetime \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df\u001b[38;5;241m.\u001b[39mtpep_pickup_datetime)\n\u001b[1;32m      7\u001b[0m     df\u001b[38;5;241m.\u001b[39mtpep_dropoff_datetime \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df\u001b[38;5;241m.\u001b[39mtpep_dropoff_datetime)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1698\u001b[0m, in \u001b[0;36mTextFileReader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   1697\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1698\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   1700\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1810\u001b[0m, in \u001b[0;36mTextFileReader.get_chunk\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1808\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m   1809\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow)\n\u001b[0;32m-> 1810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1771\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     (\n\u001b[1;32m   1775\u001b[0m         index,\n\u001b[1;32m   1776\u001b[0m         columns,\n\u001b[1;32m   1777\u001b[0m         col_dict,\n\u001b[0;32m-> 1778\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 230\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    232\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:833\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True: \n",
    "    t_start = time()\n",
    "\n",
    "    df = next(df_iter)\n",
    "\n",
    "    df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "    df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n",
    "    \n",
    "    df.to_sql(name='yellow_taxi_data', con=engine, if_exists='append')\n",
    "\n",
    "    t_end = time()\n",
    "\n",
    "    print('inserted another chunk, took %.3f second' % (t_end - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f914282b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-02-28 09:26:14--  https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 3.5.11.165, 52.217.236.8, 52.217.198.184, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|3.5.11.165|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12322 (12K) [application/octet-stream]\n",
      "Saving to: ‘taxi+_zone_lookup.csv’\n",
      "\n",
      "taxi+_zone_lookup.c 100%[===================>]  12.03K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-02-28 09:26:14 (134 MB/s) - ‘taxi+_zone_lookup.csv’ saved [12322/12322]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c129c436",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = pd.read_csv('taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cead392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zones.to_sql(name='zones', con=engine, if_exists='replace')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
